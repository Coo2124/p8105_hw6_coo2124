---
title: "p8105_hw6_coo2124"
author: "Christiana Odewumi"
date: "2024-12-01"
output: github_document
---

# Load libraries
```{r}
library(rnoaa)
library(tidyverse)
library(broom)
library(ggplot2)
library(modelr)
library(mgcv)
library(purrr)

set.seed(1)


knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1
Download and prepare the 2017 Central Park weather data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

weather_df %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_point() + 
  labs(
    title = "Relationship Between Daily Minimum and Maximum Temperatures (2017)",
    x = "Minimum Temperature (¬∞C)",
    y = "Maximum Temperature (¬∞C)"
  ) +
  theme_minimal()
```

we are interested in the distribution of two quantities estimated from these data

```{r}
compute_metrics = function(weather_sample) {
  model = lm(tmax ~ tmin, data = weather_sample)
  r_squared = glance(model)$r.squared
  coefficients = tidy(model)
  
  intercept = coefficients |>
    filter(term == "(Intercept)")|>
    pull(estimate)
  
  slope = coefficients|>
    filter(term == "tmin") |> 
    pull(estimate)
  
  log_beta_product = ifelse(intercept * slope > 0, log(intercept * slope), NA)
  
  tibble(
    r_squared = r_squared,
    log_beta_product = log_beta_product
  )
}
```

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities.
```{r}
bootstrap_results <- 
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap,as_tibble),
    results = map(models, compute_metrics),
  ) |> 
  unnest(results)

```

Plot the distribution of your estimates, and describe these in words.
```{r}
ggplot(bootstrap_results, aes(x = r_squared)) +
  geom_histogram(binwidth = 0.01, fill = "blue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of R-Squared from 5000 Bootstrap Samples",
    x = "R-Squared",
    y = "Frequency"
  ) +
  theme_minimal()

```
The histogram shows the distribution of r-square values from 5000 bootstrap samples, centered around 0.91, indicating the model consistently explains about 91% of the variability in the data. The distribution is unimodal with a slight left skew and minimal spread, suggesting the model's performance is robust and reliable across samples. Only a few samples have r-square values below 0.90 or above 0.92, further highlighting the model's stability.

```{r}
ggplot(bootstrap_results, aes(x = log_beta_product)) +
  geom_histogram(binwidth = 0.01, fill = "pink", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of log(beta0 * beta1) from 5000 Bootstrap Samples",
    x = "log(beta0 * beta1)",
    y = "Frequency"
  ) +
  theme_minimal()

```
The histogram shows the distribution of log(ùõΩÃÇ 0‚àóùõΩÃÇ 1) from 5000 bootstrap samples. The distribution is unimodal and symmetric, centered around approximately 2.02. The spread is narrow, indicating minimal variability in the logarithm of the product of the coefficients. This suggests that the relationship between the intercept and slope in the regression model is consistent and stable across bootstrap samples.

```{r}
summary_results = bootstrap_results |> 
  summarize(
    r_squared_lower = quantile(r_squared, 0.025),
    r_squared_upper = quantile(r_squared, 0.975),
    log_beta_product_lower = quantile(log_beta_product, 0.025),
    log_beta_product_upper = quantile(log_beta_product, 0.975)
  )
summary_results
```
The 95% confidence intervals are 0.894‚Äì0.927 for r-square and 1.967‚Äì2.058 for log(ùõΩÃÇ 0‚àóùõΩÃÇ) , indicating the model's reliability and consistency.


### problem 2
Load the data from GitHub
```{r}
homicides <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

cleaned_homicides <- homicides |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    solved = ifelse(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)
  ) |> 
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  )
```

For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.
```{r}

baltimore_data = cleaned_homicides |> 
  filter(city_state == "Baltimore, MD") |>
  select(solved, victim_age, victim_race, victim_sex)

baltimore_model = glm(
  solved ~ victim_age + victim_sex + victim_race,
  data = baltimore_data,
  family = "binomial"
)

save(baltimore_model, file = "./results/baltimore_model.RData")

baltimore_results = baltimore_model |>
  broom::tidy() |> 
  knitr::kable(digits = 3)

baltimore_results

```


fit logistic regression for each city and extract the adjusted odds ratio and confidence intervals for male vs female victims
```{r}
baltimore_results = 
  broom::tidy(baltimore_model, conf.int = TRUE) |>
  mutate(
    OR = exp (estimate),                   
    adjusted_conf.low = exp (conf.low),   
    adjusted_conf.high = exp(conf.high))|>
  filter(term == "victim_sexMale") |>
      select(OR, adjusted_conf.low, adjusted_conf.high) |>
  knitr::kable(digits = 3)

baltimore_results
```

The adjusted odds ratio (OR) of 0.426 indicates that male victims are significantly less likely to have their homicides solved compared to female victims, with a 57.4% lower likelihood. The 95% confidence interval (0.324‚Äì0.558) is  below 1, which shows that the result is statistically significant.


Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 
```{r}
city_results = cleaned_homicides |>
  group_by(city_state) |>
  nest() |>
  mutate(
    model = map(data, ~ glm(solved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial)), 
    results = map(model, ~ broom::tidy(.x, exponentiate = TRUE, conf.int = TRUE)) 
  ) |>
  unnest(results) |>
  filter(term == "victim_sexMale") |>
  select(city_state, term, estimate, conf.low, conf.high) 


city_results
```

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
city_results = city_results |>
  arrange(estimate)

ggplot(city_results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Adjusted Odds Ratios for Solving Homicides (Male vs Female Victims)",
    x = "City",
    y = "Adjusted Odds Ratio"
  ) +
  coord_flip() +
  theme_minimal()
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The plot shows significant variation in adjusted odds ratios (ORs) for solving homicides across cities, comparing male to female victims. Cities with ORs above 1 indicate males are more likely to have their cases solved, while ORs below 1 suggest the opposite. Wide confidence intervals in many cities highlight uncertainty. Statistically significant differences appear in cities where confidence intervals do not include 1, while others show no clear gender effect. Overall, the plot emphasizes disparities in homicide resolution rates by gender and the need for further investigation.







